{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM（大規模言語モデル）のファインチューニングの例\n",
    "\n",
    "CyberAgentが公開している Open-Calm と呼ばれるLLM（大規模言語モデル）をファインチューニングして問題を解いてみます．\n",
    "\n",
    "[ライブドアニュースコーパス](https://www.rondhuit.com/download.html#news%20corpus)を使って，ニュース記事のタイトルからカテゴリを予測するモデルを作成します．\n",
    "\n",
    "参考: https://note.com/npaka/n/na5b8e6f749ce\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブドアニュースのダウンロードと展開（必要に応じて）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# textディレクトリが存在しない場合\n",
    "if not os.path.exists('text'):\n",
    "    # ldcc-20140209.tar.gzファイルが存在しない場合\n",
    "    if not os.path.exists('ldcc-20140209.tar.gz'):\n",
    "        # ダウンロード\n",
    "        url = 'https://www.rondhuit.com/download/ldcc-20140209.tar.gz'\n",
    "        urllib.request.urlretrieve(url, 'ldcc-20140209.tar.gz')\n",
    "    \n",
    "    # tarファイルの展開\n",
    "    with tarfile.open('ldcc-20140209.tar.gz', 'r:gz') as tar:\n",
    "        tar.extractall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前学習モデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本パラメータ\n",
    "model_name = \"cyberagent/open-calm-medium\"   # モデルの名前\n",
    "peft_name  = \"lora-calm-medium\"              # 学習用モデル(PEFTモデル)の名前\n",
    "output_dir = \"lora-calm-medium-results\"      # 学習結果の出力先"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "model=AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|padding|>'}\n",
      "bos_token : <|endoftext|> , 0\n",
      "eos_token : <|endoftext|> , 0\n",
      "unk_token : <|endoftext|> , 0\n",
      "pad_token : <|padding|> , 1\n"
     ]
    }
   ],
   "source": [
    "# トークナイザのスペシャルトークンの確認\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(\"bos_token :\", tokenizer.eos_token, \",\", tokenizer.bos_token_id)\n",
    "print(\"eos_token :\", tokenizer.bos_token, \",\", tokenizer.eos_token_id)\n",
    "print(\"unk_token :\", tokenizer.unk_token, \",\", tokenizer.unk_token_id)\n",
    "print(\"pad_token :\", tokenizer.pad_token, \",\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN = 512  # 最大長\n",
    "\n",
    "# トークナイズ関数の定義\n",
    "def tokenize(prompt, tokenizer):\n",
    "    result = tokenizer(\n",
    "        prompt+\"<|endoftext|>\",  # EOSの付加\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=True,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": result[\"input_ids\"],\n",
    "        \"attention_mask\": result[\"attention_mask\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [984, 18504, 2281, 1114, 358, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# トークナイズの動作確認\n",
    "tokenize(\"私の名前はシェーマです\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "dirname2label = {\n",
    "    'dokujo-tsushin': \"独女通信\",\n",
    "    'it-life-hack': \"ITライフハック\",\n",
    "    'kaden-channel': \"家電チャンネル\",\n",
    "    'livedoor-homme': \"livedoor HOMME\",\n",
    "    'movie-enter': \"MOVIE ENTER\",\n",
    "    'peachy': \"Peachy\",\n",
    "    'smax': \"エスマックス\",\n",
    "    'sports-watch': \"Sports Watch\",\n",
    "    'topic-news': \"トピックニュース\",\n",
    "}\n",
    "\n",
    "data_all = []\n",
    "for dirname, label in dirname2label.items():\n",
    "    os.path.join('text', dirname)\n",
    "    filepaths = glob.glob(os.path.join('text', dirname, '*.txt'))\n",
    "    for filepath in filepaths:\n",
    "        text = open(filepath, 'r', encoding='utf-8').read()\n",
    "        # 内容のある最初の３文だけ取り出す\n",
    "        lines = []\n",
    "        for line in text.split('\\n')[2:]:\n",
    "            if len(line) > 0:\n",
    "                lines.append(line)\n",
    "                if len(lines) >= 3:\n",
    "                    break\n",
    "        text = '\\n'.join(lines) + '\\n'\n",
    "        data_all.append({'text': text, 'response': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "森ガールよりもリア充？ 「雲ガール」の実態に迫る\n",
      "秋深き　隣は何を　する人ぞ　——。これまで「森ガール」にはじまり「山ガール」、「釣りガール」とさまざまな流行を生みだしてきた“○○ガールズ”ブーム。2011年も本格的に秋の様相を呈してきた今、新たなガールズブームが産声を上げた。その名も「雲ガール」。これはPCやスマートフォンでインターネットを活用し、趣味や生活を楽しんでいる女子たちのこと。ちなみに「雲」は、今もっともホットなインターネットサービス「クラウド」を指すのである。\n",
      "今回、この雲ガールたちの座談会があるとの情報をキャッチ！　潜入し、雲ガールの実態をチェックしてきたので、ここに報告しよう。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_all[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "独女通信\n"
     ]
    }
   ],
   "source": [
    "print(data_all[0]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロンプト生成（正解データ）のための関数\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"{data_point['text']}\n",
    "\n",
    "### Label:\n",
    "{data_point['response']}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "森ガールよりもリア充？ 「雲ガール」の実態に迫る\n",
      "秋深き　隣は何を　する人ぞ　——。これまで「森ガール」にはじまり「山ガール」、「釣りガール」とさまざまな流行を生みだしてきた“○○ガールズ”ブーム。2011年も本格的に秋の様相を呈してきた今、新たなガールズブームが産声を上げた。その名も「雲ガール」。これはPCやスマートフォンでインターネットを活用し、趣味や生活を楽しんでいる女子たちのこと。ちなみに「雲」は、今もっともホットなインターネットサービス「クラウド」を指すのである。\n",
      "今回、この雲ガールたちの座談会があるとの情報をキャッチ！　潜入し、雲ガールの実態をチェックしてきたので、ここに報告しよう。\n",
      "\n",
      "\n",
      "### Label:\n",
      "独女通信\n"
     ]
    }
   ],
   "source": [
    "print(generate_prompt(data_all[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# dictionary -> pandas dataframe -> datasets.Dataset\n",
    "df_data_all = pd.DataFrame(data_all)\n",
    "dataset_all = Dataset.from_pandas(df_data_all)\n",
    "\n",
    "# 検証データの数\n",
    "VAL_SET_SIZE = 200\n",
    "\n",
    "# 学習データと検証データに分割\n",
    "train_val = dataset_all.train_test_split(\n",
    "    test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
    ")\n",
    "train_data = train_val[\"train\"]\n",
    "val_data = train_val[\"test\"]\n",
    "\n",
    "# プロンプトへの変換とトークナイズ\n",
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))\n",
    "val_data = val_data.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習用モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/fujie/.conda/envs/py39 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# モデルの準備\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786432 || all params: 409790464 || trainable%: 0.19191076149590441\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# LoRAのパラメータ\n",
    "lora_config = LoraConfig(\n",
    "    r= 8, \n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# モデルの前処理\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# LoRAモデルの準備\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 学習可能パラメータの確認\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "eval_steps = 200\n",
    "save_steps = 200\n",
    "logging_steps = 20\n",
    "\n",
    "# トレーナーの準備\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        num_train_epochs=10,\n",
    "        learning_rate=3e-4,\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=3,\n",
    "        push_to_hub=False,\n",
    "        auto_find_batch_size=True\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:230: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
      "  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8970' max='8970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8970/8970 2:21:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.949500</td>\n",
       "      <td>2.907751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.874000</td>\n",
       "      <td>2.867150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.878200</td>\n",
       "      <td>2.839062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.845800</td>\n",
       "      <td>2.823074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.814000</td>\n",
       "      <td>2.808615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.759700</td>\n",
       "      <td>2.799341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.827600</td>\n",
       "      <td>2.786136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.787400</td>\n",
       "      <td>2.779655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.771500</td>\n",
       "      <td>2.766935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.749900</td>\n",
       "      <td>2.761552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.737700</td>\n",
       "      <td>2.758508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.735600</td>\n",
       "      <td>2.752951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.637800</td>\n",
       "      <td>2.748573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.786700</td>\n",
       "      <td>2.746204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.655300</td>\n",
       "      <td>2.743604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.680100</td>\n",
       "      <td>2.741686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.644500</td>\n",
       "      <td>2.738016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.703400</td>\n",
       "      <td>2.739030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.669100</td>\n",
       "      <td>2.737758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.616700</td>\n",
       "      <td>2.733585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.703500</td>\n",
       "      <td>2.732289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.646800</td>\n",
       "      <td>2.729170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.689100</td>\n",
       "      <td>2.731302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.698600</td>\n",
       "      <td>2.730220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.633700</td>\n",
       "      <td>2.728575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.662600</td>\n",
       "      <td>2.727671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.578100</td>\n",
       "      <td>2.728462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.609200</td>\n",
       "      <td>2.727939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.606400</td>\n",
       "      <td>2.728383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.577400</td>\n",
       "      <td>2.724707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.678200</td>\n",
       "      <td>2.726262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.646900</td>\n",
       "      <td>2.726946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.627300</td>\n",
       "      <td>2.724046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.631700</td>\n",
       "      <td>2.726461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.696100</td>\n",
       "      <td>2.726311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.541200</td>\n",
       "      <td>2.724999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.569500</td>\n",
       "      <td>2.727818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.560700</td>\n",
       "      <td>2.726398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.707500</td>\n",
       "      <td>2.725684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.625700</td>\n",
       "      <td>2.723926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.591900</td>\n",
       "      <td>2.725610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.546300</td>\n",
       "      <td>2.725952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.627600</td>\n",
       "      <td>2.725334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.475300</td>\n",
       "      <td>2.725733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "# 学習の実行\n",
    "model.config.use_cache = False\n",
    "trainer.train() \n",
    "model.config.use_cache = True\n",
    "\n",
    "# LoRAモデルの保存\n",
    "trainer.model.save_pretrained(peft_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/fujie/.conda/envs/py39 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoXForCausalLM(\n",
       "      (gpt_neox): GPTNeoXModel(\n",
       "        (embed_in): Embedding(52096, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x GPTNeoXLayer(\n",
       "            (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): GPTNeoXAttention(\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "              (query_key_value): Linear8bitLt(\n",
       "                in_features=1024, out_features=3072, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (mlp): GPTNeoXMLP(\n",
       "              (dense_h_to_4h): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "              (dense_4h_to_h): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "              (act): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (embed_out): Linear(in_features=1024, out_features=52096, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# モデルの準備\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# トークンナイザーの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# LoRAモデルの準備\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, \n",
    "    peft_name, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 評価モード\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_prompt(data_point):\n",
    "    return f\"\"\"{data_point['text']}\n",
    "\n",
    "### Label:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキスト生成関数の定義\n",
    "def generate(instruction,input=None,maxTokens=512):\n",
    "    # 推論\n",
    "    prompt = generate_query_prompt({'text':instruction})\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_new_tokens=maxTokens, \n",
    "        do_sample=True,\n",
    "        temperature=0.7, \n",
    "        top_p=0.75, \n",
    "        top_k=40,         \n",
    "        no_repeat_ngram_size=2,\n",
    "    )\n",
    "    outputs = outputs[0].tolist()\n",
    "\n",
    "    # EOSトークンにヒットしたらデコード完了\n",
    "    if tokenizer.eos_token_id in outputs:\n",
    "        eos_index = outputs.index(tokenizer.eos_token_id)\n",
    "        decoded = tokenizer.decode(outputs[:eos_index])\n",
    "\n",
    "        # ラベル内容のみ抽出\n",
    "        sentinel = \"### Label:\"\n",
    "        sentinelLoc = decoded.find(sentinel)\n",
    "        if sentinelLoc >= 0:\n",
    "            response = decoded[sentinelLoc+len(sentinel):]\n",
    "            # print(response)\n",
    "        else:\n",
    "            print('Warning: Expected prompt template to be emitted.  Ignoring output.')\n",
    "            response = None\n",
    "    else:\n",
    "        print('Warning: no <eos> detected ignoring output')\n",
    "        response = None\n",
    "        decoded = None\n",
    "\n",
    "    return response, prompt, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- PROMPT ----\n",
      "ウルトラブックとは一体何なのか？パソコン市場は縮小するのか【デジ通】\n",
      "2011年の6月にそのコンセプトが発表されたUltrabook(ウルトラブック)だが、2011年末から2012年初頭にかけて、Ultrabook第1弾と呼べる製品が多数登場し、市場でも一定の人気を得ているようだ。\n",
      "今後も新製品が多数予定されているが、Ultrabookの本命は2013年にインテルから登場する新プロセッサHaswell(コード名)を採用する製品と言われており、それまでにどう成長していくか注目される。\n",
      "\n",
      "\n",
      "### Label:\n",
      "\n",
      "---- RESPONSE ----\n",
      "\n",
      "ITライフ!業界ニュース\n",
      "---- TARGET ----\n",
      "ITライフハック\n"
     ]
    }
   ],
   "source": [
    "# d = val_data[33]\n",
    "d = data_all[1700]\n",
    "\n",
    "response, prompt, decoded = generate(d['text'])\n",
    "print(\"---- PROMPT ----\")\n",
    "print(prompt)\n",
    "print(\"---- RESPONSE ----\")\n",
    "print(response)\n",
    "print(\"---- TARGET ----\")\n",
    "print(d['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- PROMPT ----\n",
      "藻類から燃料、CO2実質ゼロ　ちとせ研究所が大型施設\n",
      "温暖化ガスの排出が実質ゼロの燃料を生物の力で作る取り組みが進む。\n",
      "ちとせ研究所（川崎市）は4月、マレーシアに二酸化炭素（CO2）から燃料の原料を作る藻類を培養する世界最大規模の施設を開設した。\n",
      "将来的に製造コストを化石燃料と競争できる水準に抑えることを目指す。\n",
      "\n",
      "\n",
      "### Label:\n",
      "\n",
      "---- RESPONSE ----\n",
      "\n",
      "トピックニュース\n",
      "---- TARGET ----\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "d = dict(text = \"\"\"藻類から燃料、CO2実質ゼロ　ちとせ研究所が大型施設\n",
    "温暖化ガスの排出が実質ゼロの燃料を生物の力で作る取り組みが進む。\n",
    "ちとせ研究所（川崎市）は4月、マレーシアに二酸化炭素（CO2）から燃料の原料を作る藻類を培養する世界最大規模の施設を開設した。\n",
    "将来的に製造コストを化石燃料と競争できる水準に抑えることを目指す。\n",
    "\"\"\",\n",
    "response = \"unknown\")\n",
    "\n",
    "response, prompt, decoded = generate(d['text'])\n",
    "print(\"---- PROMPT ----\")\n",
    "print(prompt)\n",
    "print(\"---- RESPONSE ----\")\n",
    "print(response)\n",
    "print(\"---- TARGET ----\")\n",
    "print(d['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fujie/.conda/envs/py39/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:230: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
      "  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- PROMPT ----\n",
      "アラサーの乾燥におすすめの化粧水ランキング8選！プチプラからデパコスまで紹介\n",
      "「アラサー乾燥肌におすすめの化粧水はどれ？」とお悩みではありませんか？\n",
      "そこで今回は、アラサー女子向けで乾燥になる方向けにおすすめ化粧水を紹介します。\n",
      "ドラッグストアなどで買えるプチプラ化粧水からデパコス化粧水まで幅広く解説！\n",
      "\n",
      "\n",
      "### Label:\n",
      "\n",
      "---- RESPONSE ----\n",
      "\n",
      "Peachy\n",
      "---- TARGET ----\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "d = dict(text = \"\"\"アラサーの乾燥におすすめの化粧水ランキング8選！プチプラからデパコスまで紹介\n",
    "「アラサー乾燥肌におすすめの化粧水はどれ？」とお悩みではありませんか？\n",
    "そこで今回は、アラサー女子向けで乾燥になる方向けにおすすめ化粧水を紹介します。\n",
    "ドラッグストアなどで買えるプチプラ化粧水からデパコス化粧水まで幅広く解説！\n",
    "\"\"\",\n",
    "response = \"unknown\")\n",
    "\n",
    "response, prompt, decoded = generate(d['text'])\n",
    "print(\"---- PROMPT ----\")\n",
    "print(prompt)\n",
    "print(\"---- RESPONSE ----\")\n",
    "print(response)\n",
    "print(\"---- TARGET ----\")\n",
    "print(d['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

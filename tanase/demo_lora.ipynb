{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本パラメータ\n",
    "model_name = \"cyberagent/open-calm-3b\"   # モデルの名前\n",
    "peft_name  = \"lora-calm-3b\"              # 学習用モデル(PEFTモデル)の名前\n",
    "output_dir = \"lora-calm-3b-results\"      # 学習結果の出力先"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "WARNING: BNB_CUDA_VERSION=123 environment variable detected; loading libbitsandbytes_cuda123.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoXForCausalLM(\n",
       "      (gpt_neox): GPTNeoXModel(\n",
       "        (embed_in): Embedding(52224, 2560)\n",
       "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x GPTNeoXLayer(\n",
       "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (attention): GPTNeoXSdpaAttention(\n",
       "              (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "              (query_key_value): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2560, out_features=7680, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=7680, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (mlp): GPTNeoXMLP(\n",
       "              (dense_h_to_4h): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
       "              (dense_4h_to_h): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
       "              (act): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (embed_out): Linear(in_features=2560, out_features=52224, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# モデルの準備\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# トークンナイザーの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# LoRAモデルの準備\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, \n",
    "    # peft_name, \n",
    "    output_dir + \"/checkpoint-1140\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 評価モード\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_prompt(data_point):\n",
    "    if data_point['関係条件'].startswith('先生'):\n",
    "        role = \"先生\"\n",
    "    else:\n",
    "        role = \"後輩\"\n",
    "    if data_point['口調条件'].startswith('敬語'):\n",
    "        tone = \"敬語\"\n",
    "    else:\n",
    "        tone = \"タメ口\"\n",
    "\n",
    "    # roleによって少し様子が変わりそうなので入れることにしました．\n",
    "    # toneは使わない方がよさそうなので外しました．\n",
    "\n",
    "    prompt = f\"\"\"role: {role}\n",
    "content: {data_point['内容']} \n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    # print(prompt)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキスト生成関数の定義\n",
    "def generate(instruction,role='先生', input=None,maxTokens=512):\n",
    "    # 推論\n",
    "    prompt = generate_query_prompt({'内容':instruction, '関係条件':role, '口調条件': ''})\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_new_tokens=maxTokens, \n",
    "        do_sample=True,\n",
    "        temperature=0.7, \n",
    "        top_p=0.75, \n",
    "        top_k=40,         \n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    outputs = outputs[0].tolist()\n",
    "\n",
    "    # EOSトークンにヒットしたらデコード完了\n",
    "    if tokenizer.eos_token_id in outputs:\n",
    "        eos_index = outputs.index(tokenizer.eos_token_id)\n",
    "        decoded = tokenizer.decode(outputs[:eos_index])\n",
    "\n",
    "        # ラベル内容のみ抽出\n",
    "        sentinel = \"### Response:\\n\"\n",
    "        sentinelLoc = decoded.find(sentinel)\n",
    "        if sentinelLoc >= 0:\n",
    "            response = decoded[sentinelLoc+len(sentinel):]\n",
    "            # print(response)\n",
    "        else:\n",
    "            print('Warning: Expected prompt template to be emitted.  Ignoring output.')\n",
    "            response = None\n",
    "    else:\n",
    "        print('Warning: no <eos> detected ignoring output')\n",
    "        response = None\n",
    "        decoded = None\n",
    "\n",
    "    return response, prompt, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('テスト',\n",
       " 'role: 先生\\ncontent: テスト \\n\\n### Response:\\n',\n",
       " 'role: 先生\\ncontent: テスト \\n\\n### Response:\\nテスト')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"テスト\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset_dict = DatasetDict.load_from_disk('dataset_tokenized_simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_test = dataset_dict[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- PROMPT ----\n",
      "role: 先生\n",
      "content: どういたしまして！間違いを見つけて解決できて良かったね。他にも何か質問があれば気軽に聞いてね。 \n",
      "\n",
      "### Response:\n",
      "\n",
      "---- RESPONSE ----\n",
      "どういまして!間違えを見つけてくれて良かったですね。他に何か聞きたいことあれば気軽に聞いちゃってください。\n",
      "---- TARGET ----\n",
      "どういたしまして！間違いを見つけて解決できて良かったね。他にも何か質問があれば気軽に聞いてね。\n"
     ]
    }
   ],
   "source": [
    "# data = dataset_dict[\"test\"][46]\n",
    "data = data_set_test[2]\n",
    "\n",
    "response, prompt, decoded = generate(data['内容'], data['関係条件'])\n",
    "print(\"---- PROMPT ----\")\n",
    "print(prompt)\n",
    "print(\"---- RESPONSE ----\")\n",
    "print(response)\n",
    "print(\"---- TARGET ----\")\n",
    "print(data['添削'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for d in data_set_test:\n",
    "    response, prompt, decoded = generate(d['内容'], d['関係条件'])\n",
    "    results.append({\n",
    "        'conv_id': d['会話ID'],\n",
    "        'utt_id': d['発話ID'],\n",
    "        'role': d['関係条件'],\n",
    "        'text': d['内容'],\n",
    "        'target': d['添削'],\n",
    "        'predicted': response,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_excel('valid-results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_train = dataset_dict[\"train\"]\n",
    "\n",
    "results = []\n",
    "for d in data_set_train:\n",
    "    response, prompt, decoded = generate(d['内容'], d['関係条件'])\n",
    "    results.append({\n",
    "        'conv_id': d['会話ID'],\n",
    "        'utt_id': d['発話ID'],\n",
    "        'role': d['関係条件'],\n",
    "        'text': d['内容'],\n",
    "        'target': d['添削'],\n",
    "        'predicted': response,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_excel('train-results.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
